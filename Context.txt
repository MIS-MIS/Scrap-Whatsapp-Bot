================================================================================
                    GOOGLE MAPS SCRAPER BOT - COMPLETE CONTEXT
================================================================================

PROJECT OVERVIEW:
================================================================================
This is a comprehensive Google Maps business scraper bot with advanced scheduling
capabilities. The system extracts business data (name, address, phone, website,
reviews) from Google Maps and provides automated scheduling with intelligent
conflict resolution.

MAIN OBJECTIVE:
- Scrape business data from Google Maps
- Provide user-friendly interface for both manual and automated scraping
- Implement smart scheduling system with conflict prevention
- Maintain single-file simplicity for users

================================================================================
                              FILE STRUCTURE
================================================================================

CORE FILES:
-----------
1. main.py (713 lines)
   - Primary scraping engine using Playwright
   - Business data extraction with multiple fallback selectors
   - Smart duplicate prevention and caching system
   - Resume capability for interrupted sessions
   - Indian phone number formatting (+91 format)
   - Command-line interface with extensive options

2. interactive_scraper.py (ENHANCED - 1400+ lines)
   - ALL-IN-ONE user interface (no external files needed)
   - Original scraping features (Quick, Custom, Batch searches)
   - Complete integrated scheduling system
   - Multi-schedule management with conflict detection
   - Real-time scheduler with sequential execution
   - Import/Export functionality
   - Comprehensive help and troubleshooting

3. fetched_contacts.json
   - Cache file storing unique IDs of processed businesses
   - Prevents duplicate scraping across sessions
   - Currently contains 86 cached contact IDs
   - Format: ["business_name_address_phone", ...]

4. search_progress.json
   - Tracks scroll positions for each search query
   - Enables resuming from last position if interrupted
   - Format: {"search_query": last_position}
   - Example: {"test business delhi": 6, "Architects Delhi": 100}

5. multi_schedules.json (NEW)
   - Stores all schedule configurations
   - Supports multiple simultaneous schedules
   - Includes conflict detection metadata
   - Auto-generated by scheduling system

6. output/all_contacts.csv
   - Centralized database of all scraped contacts
   - Appends new data without overwriting existing
   - Includes search query tracking for each contact

DEPRECATED/STANDALONE FILES (Now integrated into interactive_scraper.py):
------------------------------------------------------------------------
- scheduler_scraper.py (473 lines) - Dedicated scheduling interface
- conflict_checker.py - Conflict detection tool
- multi_scheduler.py - Advanced multi-schedule manager
- schedule_demo.py - Demonstration script

================================================================================
                            TECHNICAL ARCHITECTURE
================================================================================

CORE CLASSES:
-------------

1. Business (dataclass) - Lines 11-55 in main.py
   - Holds business data: name, address, website, phone, reviews, search_query
   - get_unique_id(): Generates unique identifier for duplicate detection
   - format_indian_phone(): Standardizes phone numbers to +91 format
   - Handles scientific notation conversion and various input formats

2. BusinessList (dataclass) - Lines 57-120 in main.py
   - Manages collections of Business objects
   - dataframe(): Converts to pandas DataFrame
   - append_to_centralized_csv(): Smart data persistence without overwriting
   - save_to_excel/csv(): Legacy export methods

3. ContactManager - Lines 123-181 in main.py
   - Handles caching and duplicate prevention
   - load/save_cache(): Manages fetched_contacts.json
   - load/save_search_progress(): Manages search_progress.json
   - is_already_fetched(): Duplicate checking
   - get/update_search_position(): Resume capability

4. Schedule (dataclass) - Lines 17-32 in interactive_scraper.py
   - Individual schedule configuration
   - Fields: id, name, search_terms, start_time, duration_minutes, etc.
   - auto_sequence: Flag for sequential execution
   - currently_running: Prevents simultaneous execution

5. InteractiveScraper - Main class in interactive_scraper.py
   - Complete user interface and scheduling system
   - Integrates all functionality in single class
   - Handles menu navigation, schedule management, conflict resolution

================================================================================
                              KEY FEATURES
================================================================================

SCRAPING CAPABILITIES:
----------------------
1. Multi-selector data extraction with fallbacks
2. Character encoding fixes for international text
3. Business name validation to filter UI elements
4. Phone number standardization (Indian +91 format)
5. Review count and rating extraction
6. Website and address extraction
7. Smart scrolling with efficiency tracking
8. Duplicate prevention during collection and processing
9. Resume capability for interrupted sessions
10. Rate limiting and anti-detection measures

SCHEDULING SYSTEM:
------------------
1. Quick Schedules (one-time runs)
2. Recurring Schedules (automated intervals)
3. Multi-schedule support with conflict detection
4. Smart time suggestions based on current time
5. Auto-sequencing for conflicting schedules
6. Real-time execution monitoring
7. Manual testing capability
8. Import/Export for backup/restore
9. Schedule editing and management
10. Comprehensive conflict resolution

CONFLICT RESOLUTION:
--------------------
1. Automatic conflict detection during schedule creation
2. Three-strategy time suggestion algorithm:
   - Strategy 1: Find slots BEFORE conflicting schedules
   - Strategy 2: Find slots AFTER conflicting schedules
   - Strategy 3: Fallback slots around current time
3. Four resolution options when conflicts detected:
   - Use suggested safe time
   - Auto-sequence (calculate optimal time)
   - Proceed with conflict (enable auto-sequencing)
   - Cancel schedule
4. Sequential execution during runtime
5. Real-time conflict prevention

================================================================================
                           SCHEDULING ALGORITHMS
================================================================================

TIME SUGGESTION ALGORITHM:
--------------------------
1. get_safe_time_suggestions():
   - Analyzes current time and existing schedules
   - Identifies conflicting schedules
   - Applies three-strategy approach for suggestions
   - Sorts by proximity to current time
   - Returns top 8 relevant suggestions

2. find_slots_before_conflicts():
   - Finds earliest conflicting schedule
   - Calculates latest safe start time
   - Generates 30-minute slots before conflict
   - Ensures suggestions are after current time
   - Adds 10-minute buffer for safety

3. find_slots_after_conflicts():
   - Finds latest ending conflicting schedule
   - Adds buffer time after completion
   - Generates slots until end of day (22:00)
   - Rounds to 30-minute intervals
   - Validates each suggestion for safety

4. calculate_sequential_time():
   - Finds latest end time of all conflicting schedules
   - Adds 10-minute buffer
   - Handles day rollover for late schedules
   - Returns optimal sequential start time

EXECUTION ALGORITHM:
--------------------
1. start_scheduler():
   - Monitors active schedules continuously
   - Checks for scheduled run times every second
   - Tracks currently running schedules
   - Implements auto-sequencing logic
   - Prevents simultaneous conflicting executions

2. execute_schedule():
   - Marks schedule as currently_running
   - Executes all search terms sequentially
   - Tracks success/failure for each search
   - Updates schedule statistics
   - Resets auto_sequence flags
   - Notifies about queued schedules

================================================================================
                              USER WORKFLOWS
================================================================================

BASIC SCRAPING WORKFLOW:
-------------------------
1. Run: python interactive_scraper.py
2. Choose scraping option (1-3):
   - Quick Search: Default 100 contacts with duplicate skipping
   - Custom Search: User-defined parameters
   - Batch Search: Multiple searches in sequence
3. Enter search terms and parameters
4. System executes scraping with real-time progress
5. Data saved to output/all_contacts.csv

SCHEDULING WORKFLOW:
--------------------
1. Run: python interactive_scraper.py
2. Choose: 4 (Scheduled Scraping)
3. Choose scheduling option (1-8):
   - Quick Schedule: One-time run at specific time
   - Recurring Schedule: Automated runs at intervals
   - View All Schedules: Status and configuration
   - Start Scheduled Scraping: Begin automation
   - Manual Run: Test immediately
   - Schedule Management: Edit/delete/import/export
   - Conflict Checker: Analyze potential conflicts
   - Schedule Help: Examples and documentation

CONFLICT RESOLUTION WORKFLOW:
-----------------------------
1. User creates schedule with conflicting time
2. System detects conflicts automatically
3. System shows conflict details and safe time suggestions
4. User chooses resolution option:
   a. Use suggested safe time → Schedule updated
   b. Auto-sequence → Time calculated automatically
   c. Proceed with conflict → Auto-sequencing enabled
   d. Cancel → Schedule creation aborted
5. System saves schedule with appropriate settings
6. During execution, conflicts handled automatically

================================================================================
                            TECHNICAL DETAILS
================================================================================

DATA EXTRACTION SELECTORS:
---------------------------
Business Name (multiple fallbacks):
- '//h1[contains(@class, "DUwDvf")]'
- '//h1[@data-attrid="title"]'
- '//div[contains(@class, "lMbq3e")]//div[contains(@class, "fontHeadlineSmall")]'
- '//div[contains(@class, "x3AX1-LfntMc-header-title")]//span'
- '//h1[contains(@class, "fontHeadlineLarge")]'
- '//div[@role="main"]//h1'
- '//div[contains(@class, "SPZz6b")]//h1'

Address:
- '//button[@data-item-id="address"]//div[contains(@class, "fontBodyMedium")]'

Website:
- '//a[@data-item-id="authority"]//div[contains(@class, "fontBodyMedium")]'

Phone:
- '//button[contains(@data-item-id, "phone:tel:")]//div[contains(@class, "fontBodyMedium")]'

Reviews:
- '//button[@jsaction="pane.reviewChart.moreReviews"]//span'
- '//div[@jsaction="pane.reviewChart.moreReviews"]//div[@role="img"]'

PHONE NUMBER FORMATTING:
-------------------------
format_indian_phone() handles:
- 10 digits: 9876543210 → +919876543210
- 11 digits with 0: 01123456789 → +91123456789
- 12 digits with 91: 919876543210 → +919876543210
- Scientific notation conversion
- Non-digit character removal

EXECUTION TIME ESTIMATION:
---------------------------
estimate_execution_time():
- Base: 0.1 minutes per contact (6 seconds)
- Buffer: max(5, search_terms * 2) minutes
- Formula: (search_terms * limit_per_run * 0.1) + buffer

DUPLICATE PREVENTION:
---------------------
1. Real-time during collection using business names
2. Persistent across sessions using unique IDs
3. Unique ID format: "name_address_phone" (lowercase, spaces to underscores)
4. Cache stored in fetched_contacts.json
5. Efficiency tracking and reporting

================================================================================
                              MENU STRUCTURE
================================================================================

MAIN MENU (interactive_scraper.py):
------------------------------------
1. Quick Search (Default: 100 contacts)
2. Custom Search (Set your parameters)
3. Batch Search (Multiple searches)
4. 🕐 Scheduled Scraping (ALL-IN-ONE) ← INTEGRATED SYSTEM
5. Cache Management
6. View Statistics & Files
7. Troubleshooting Tools
8. Settings & Configuration
9. Help & Commands Reference
10. Exit

SCHEDULED SCRAPING SUBMENU:
----------------------------
1. Quick Schedule (Run once at specific time)
2. Recurring Schedule (Run at intervals)
3. View All Schedules
4. Start Scheduled Scraping
5. Manual Run (Test immediately)
6. Schedule Management
7. Conflict Checker
8. Schedule Help & Examples
9. Back to Main Menu

SCHEDULE MANAGEMENT SUBMENU:
-----------------------------
1. Edit schedule
2. Delete schedule
3. Activate/Deactivate schedule
4. Export schedules
5. Import schedules
6. Back to scheduler menu

================================================================================
                           COMMAND LINE OPTIONS
================================================================================

MAIN.PY COMMAND LINE ARGUMENTS:
--------------------------------
-s, --search: Search query for Google Maps
-l, --limit: Number of contacts to fetch (default: 100)
--skip-duplicates: Skip already fetched contacts
--clear-cache: Clear the cache of fetched contacts
--clear-progress: Clear search progress (start from beginning)
--min-delay: Minimum delay between requests (default: 2.0)
--max-delay: Maximum delay between requests (default: 5.0)

EXAMPLE COMMANDS:
-----------------
python main.py -s "restaurants delhi" --limit 200 --skip-duplicates
python main.py -s "hotels mumbai" --limit 100 --min-delay 3 --max-delay 8
python main.py --clear-cache --clear-progress -s "test" --limit 1

================================================================================
                              FILE FORMATS
================================================================================

SCHEDULE CONFIGURATION (multi_schedules.json):
-----------------------------------------------
[
  {
    "id": "quick_1703123456",
    "name": "Quick Schedule - 13:13",
    "search_terms": ["Restaurant in Delhi"],
    "start_time": "13:13",
    "duration_minutes": 0,
    "limit_per_run": 10,
    "skip_duplicates": true,
    "is_recurring": false,
    "is_active": true,
    "last_run": "2025-08-20 13:13:00",
    "total_runs": 1,
    "estimated_duration_minutes": 6,
    "auto_sequence": false,
    "currently_running": false
  }
]

CONTACT CACHE (fetched_contacts.json):
--------------------------------------
[
  "r.k._architects_&_engineers_118,_local_shopping_centre,_block_c,_vikaspuri,_new_delhi,_delhi_110018_+919650215056",
  "deep_architects_a-133,_near_tanishq_showroom,_block_a,_vishal_enclave,_rajouri_garden,_new_delhi,_delhi,_110027_+919643965929"
]

SEARCH PROGRESS (search_progress.json):
---------------------------------------
{
  "test business delhi": 6,
  "Architects Delhi": 100,
  "Restaurant in Delhi": 11
}

OUTPUT CSV (output/all_contacts.csv):
------------------------------------
name,address,website,phone_number,reviews_count,reviews_average,search_query
"Daryaganj Restaurant","Daryaganj, New Delhi","","+919355771947",1250,4.2,"Restaurant in Delhi"
"Pakwan Restaurant","Karol Bagh, New Delhi","","+911142137987",890,4.1,"Restaurant in Delhi"

================================================================================
                           CONFLICT SCENARIOS
================================================================================

SCENARIO 1: DIRECT TIME CONFLICT
---------------------------------
Existing: Schedule A at 15:30
New: Schedule B at 15:30
Detection: Direct time conflict
Resolution Options:
1. Use safe time (16:00, 16:30, 17:00, etc.)
2. Auto-sequence (15:45 after A completes)
3. Proceed with conflict (auto-sequencing enabled)
4. Cancel

SCENARIO 2: EXECUTION OVERLAP
------------------------------
Existing: Schedule A at 15:30 (duration: 25 min, ends 15:55)
New: Schedule B at 15:35 (duration: 15 min, ends 15:50)
Detection: Execution overlap (15:35 falls within 15:30-15:55)
Resolution: Same options as above

SCENARIO 3: MULTIPLE CONFLICTS
-------------------------------
Existing: Schedule A at 15:30, Schedule B at 16:00
New: Schedule C at 15:45
Detection: Overlaps with both A and B
Resolution: Auto-sequence to 16:30 (after both complete)

SCENARIO 4: CURRENT TIME CONSIDERATIONS
---------------------------------------
Current: 12:30
Existing: Schedule at 13:00
New: Schedule at 13:00
Smart Suggestions:
- Before conflict: 12:30 (if enough time for execution)
- After conflict: 13:30, 14:00, 14:30
- No early morning suggestions (6:00 AM eliminated)

================================================================================
                              ERROR HANDLING
================================================================================

SCRAPING ERRORS:
----------------
1. Browser launch failures → Retry mechanism
2. Page load timeouts → Extended timeout handling
3. Element not found → Multiple selector fallbacks
4. Rate limiting → Automatic delay adjustment
5. Network issues → Graceful degradation
6. Data extraction failures → Continue with next listing

SCHEDULING ERRORS:
------------------
1. Invalid time format → User re-prompt with validation
2. Schedule file corruption → Graceful recovery with defaults
3. Execution failures → Mark as not running, continue monitoring
4. Conflict detection errors → Safe fallback to manual resolution
5. Auto-sequence calculation errors → Default to manual time selection

FILE SYSTEM ERRORS:
-------------------
1. Missing output directory → Automatic creation
2. Permission issues → Clear error messages
3. Disk space issues → Graceful handling
4. File corruption → Backup and recovery mechanisms

================================================================================
                            PERFORMANCE METRICS
================================================================================

SCRAPING PERFORMANCE:
---------------------
- Average: 6 seconds per contact
- Efficiency tracking: (collected/examined) * 100
- Duplicate detection rate: Real-time reporting
- Success rate: Per search and overall tracking
- Resume capability: Position-based recovery

SCHEDULING PERFORMANCE:
-----------------------
- Conflict detection: Sub-second analysis
- Time suggestion generation: < 1 second
- Schedule execution monitoring: 1-second intervals
- Status updates: Every 30 seconds during execution
- Memory usage: Minimal (schedule data in memory)

SYSTEM REQUIREMENTS:
--------------------
- Python 3.7+
- Playwright (browser automation)
- Pandas (data manipulation)
- Required disk space: ~50MB for dependencies
- RAM usage: ~200-500MB during execution
- Network: Stable internet connection required

================================================================================
                              BEST PRACTICES
================================================================================

SCHEDULING BEST PRACTICES:
---------------------------
1. Minimum 30-minute gaps between schedules
2. Use auto-sequencing for optimal timing
3. Test with manual runs before automation
4. Regular cache cleanup for optimal performance
5. Monitor execution times to adjust estimates
6. Use conflict checker before creating multiple schedules
7. Export schedules for backup before major changes

SCRAPING BEST PRACTICES:
------------------------
1. Use reasonable limits (100-500 contacts per search)
2. Enable duplicate skipping for efficiency
3. Use appropriate delays (2-5 seconds) to avoid blocking
4. Monitor for rate limiting and adjust delays accordingly
5. Regular cache cleanup to prevent excessive growth
6. Use specific search terms for better data quality
7. Schedule during off-peak hours when possible

DATA MANAGEMENT:
----------------
1. Regular backup of output/all_contacts.csv
2. Monitor file sizes and rotate if necessary
3. Clean up old cache files periodically
4. Export schedules before system changes
5. Validate data quality regularly
6. Use search query tracking for data organization

================================================================================
                              TROUBLESHOOTING
================================================================================

COMMON ISSUES AND SOLUTIONS:
-----------------------------

1. "No schedules available" Error:
   - Solution: Create a schedule first using options 1 or 2
   - Check: View All Schedules to see current status

2. Early morning time suggestions (6:00 AM):
   - Fixed: New smart algorithm considers current time
   - Use: Auto-sequence option for better timing

3. Browser conflicts during execution:
   - Solution: Auto-sequencing prevents simultaneous execution
   - Check: Only one schedule runs at a time

4. Schedule not running at specified time:
   - Check: System time accuracy
   - Verify: Schedule is active (not deactivated)
   - Ensure: Scheduler is running (option 4)

5. Data not saving:
   - Check: Output directory permissions
   - Verify: Disk space availability
   - Ensure: No file locks on CSV files

6. High duplicate rates:
   - Solution: Clear cache if area fully scraped
   - Try: Different search terms or locations
   - Use: More specific search queries

7. Slow execution:
   - Adjust: Increase delays if being rate limited
   - Check: Network connection stability
   - Monitor: System resource usage

DIAGNOSTIC COMMANDS:
--------------------
1. Check system requirements: Option 7 → 4
2. View cache status: Option 5
3. Test mode: Option 7 → 1 (5 contacts only)
4. Stealth mode: Option 7 → 2 (long delays)
5. Conflict analysis: Option 4 → 7

================================================================================
                              FUTURE ENHANCEMENTS
================================================================================

PLANNED IMPROVEMENTS:
---------------------
1. Web-based dashboard for remote monitoring
2. Email notifications for schedule completion
3. Advanced filtering and search capabilities
4. Integration with CRM systems
5. Multi-language support for international markets
6. Advanced analytics and reporting
7. Cloud deployment options
8. API endpoints for external integration

SCALABILITY CONSIDERATIONS:
---------------------------
1. Database backend for large datasets
2. Distributed scraping across multiple instances
3. Load balancing for high-volume operations
4. Caching strategies for improved performance
5. Monitoring and alerting systems
6. Automated backup and recovery

================================================================================
                              VERSION HISTORY
================================================================================

DEVELOPMENT PHASES:
-------------------

Phase 1: Basic Scraping (main.py)
- Core scraping functionality
- Business data extraction
- Basic duplicate prevention
- Command-line interface

Phase 2: User Interface (interactive_scraper.py)
- Menu-driven interface
- User-friendly workflows
- Troubleshooting tools
- Help system

Phase 3: Scheduling System
- Quick and recurring schedules
- Basic time-based execution
- Simple conflict detection
- Manual schedule management

Phase 4: Advanced Scheduling
- Multi-schedule support
- Intelligent conflict resolution
- Auto-sequencing capability
- Smart time suggestions

Phase 5: Integration and Optimization
- Single-file solution
- Enhanced conflict algorithms
- Real-time execution monitoring
- Comprehensive error handling

CURRENT VERSION: Phase 5 Complete
- All functionality integrated into interactive_scraper.py
- Smart conflict resolution with three-strategy algorithm
- Auto-sequencing for seamless execution
- Context-aware time suggestions
- Production-ready system

================================================================================
                              USAGE STATISTICS
================================================================================

CURRENT SYSTEM STATE:
---------------------
- Total contacts in cache: 86
- Active schedules: Variable (user-dependent)
- Successful execution rate: High (with conflict resolution)
- User satisfaction: Improved with single-file approach

EXAMPLE USAGE SESSION:
----------------------
User creates schedule at 13:13 for "Restaurant in Delhi"
- Execution time: 13:13:00
- Search terms: 1
- Contacts requested: 10
- Contacts found: 10
- Success rate: 100%
- Actual duration: 1 minute
- Estimated duration: 6 minutes (conservative estimate)
- Efficiency: 90.9% (10 collected from 11 examined)

================================================================================
                                CONCLUSION
================================================================================

This Google Maps Scraper Bot represents a complete, production-ready solution
for automated business data collection with advanced scheduling capabilities.

KEY ACHIEVEMENTS:
-----------------
✅ Single-file simplicity for users (interactive_scraper.py)
✅ Comprehensive scraping with anti-detection measures
✅ Intelligent scheduling with conflict resolution
✅ Auto-sequencing for seamless execution
✅ Context-aware time suggestions
✅ Real-time monitoring and status updates
✅ Robust error handling and recovery
✅ User-friendly interface with extensive help
✅ Production-ready performance and reliability

The system successfully addresses the original requirements while providing
enterprise-level features in a user-friendly package. Users need only remember
one command: "python interactive_scraper.py" to access all functionality.

The smart conflict resolution system ensures schedules never interfere with
each other, providing reliable automated data collection for business
intelligence and market research purposes.

================================================================================
                              END OF CONTEXT
================================================================================

Last Updated: 2025-01-20
Total Lines of Context: 500+
System Status: Production Ready
User Experience: Optimized for Simplicity
Technical Complexity: Hidden Behind User-Friendly Interface

================================================================================